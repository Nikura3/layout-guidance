# Training-Free Layout Control with Cross-Attention Guidance
[Minghao Chen](https://silent-chen.github.io), [Iro Laina](https://campar.in.tum.de/Main/IroLaina), [Andrea Vedaldi](https://www.robots.ox.ac.uk/~vedaldi/)

[[Paper]()] [[Project Page](https://silent-chen.github.io/layout-guidance/)] [[Demo]()]

https://user-images.githubusercontent.com/30588507/229642269-57527ded-3189-4aa2-9590-1f3de4d51cad.mp4


Our method manage to control of layout of images generated by large pretrained Text-to-Image diffusion models **without training** through the layout guidance performed on the cross-attention maps.
<div align="center">
    <img width="100%" alt="teaser" src="https://github.com/silent-chen/layout-guidance/blob/gh-page/resources/teaser.png?raw=true"/>
</div>

## Abstract
Recent diffusion-based generators can produce high-quality images based only on textual prompts. However, they do not correctly interpret instructions that specify the spatial layout of the composition. We propose a simple approach that can achieve robust layout control without requiring training or fine-tuning the image generator. Our technique, which we call layout guidance, manipulates the cross-attention layers that the model uses to interface textual and visual information, and steers the reconstruction in the desired direction given, e.g., a user-specified layout. In order to determine how to best guide attention, we study the role of different attention maps for generating an image and experiment with two alternative strategies, forward and backward guidance. We evaluate our method quantitatively and qualitatively in several experiments, validating its effectiveness. We further demonstrate its flexibility by applying it to models that use image generators for various tasks; in particular, we demonstrate how this technique can be used to edit the layout of a given real image. 

## Environment Setup

To set up the enviroment you can easily run the following command:
```buildoutcfg
conda create -n layout-guidance python=3.8
conda activate layout-guidance
pip install -r requirements.txt
```

## Inference 

We provide an example inference script. The example outputs, including log file, generated images, config file,  are saved to the specified path `./example_output`.  Detail configuration can be found in the `./conf/base_config.yaml` and `inference.py`.
```buildoutcfg
python inference.py genenral.save_path=./example_output 
```

## Examples

### Real Image Editting
We achieve real image editing based on Dreambooth and Text Inversion. Specifically, we can change the context, location and size of the objects in the original image.
<div align="center">
    <img width="90%" alt="teaser" src="https://github.com/silent-chen/layout-guidance/blob/gh-page/resources/real_image_editing.png?raw=true"/>
</div>


## Citation

If this repo is helpful for you, please consider to cite it. Thank you! :)

```bibtex
@InProceedings{chen2023layout,
  author    = {Minghao Chen and Iro Laina and Andrea Vedaldi},
  title     = {Training-Free Layout Control with Cross-Attention Guidance},
  journal   = {arxiv},
  year      = {2023.3}
}
```

## To Do List

- [x] Basic Backward Guidance
- [ ] Support Different Layer of Backward Guidance
- [ ] Forward Guidance
- [ ] Real Image Editting Example

## Acknowledgements

This research is supported by ERC-CoG UNION 101001212. 
The codes are inspired by [Diffuser](https://github.com/huggingface/diffusers) and [Stable Diffusion](https://github.com/CompVis/stable-diffusion).